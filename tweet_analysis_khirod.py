# -*- coding: utf-8 -*-
"""Tweet Analysis_Khirod.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bzeAqUZpfHXvdsL-pPcTr8M8lb4VMg4
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/Twitter.csv', encoding='latin-1')
df.head()

# Preprocessing
df = df[['Text', 'Brand', 'Sentiment']]  # Select relevant columns
df.dropna(inplace=True)  # Remove rows with missing values

# Encode categorical features
le_brand = LabelEncoder()
df['Brand'] = le_brand.fit_transform(df['Brand'])
le_sentiment = LabelEncoder()
df['Sentiment'] = le_sentiment.fit_transform(df['Sentiment'])

# Tokenize text data
max_words = 10000  # Adjust as needed
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(df['Text'])
sequences = tokenizer.texts_to_sequences(df['Text'])

# Pad sequences to ensure uniform length
max_sequence_length = 100  # Adjust as needed
padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)

# Split data into training and testing sets
X = np.array(padded_sequences)
y = np.array(df['Sentiment'])
brand = np.array(df['Brand']) # Include Brand as a feature
X_train, X_test, y_train, y_test, brand_train, brand_test = train_test_split(X, y, brand, test_size=0.2, random_state=42)

# Model building
model = Sequential()
model.add(Embedding(max_words, 128, input_length=max_sequence_length))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2)) #added dropout for regularization
model.add(Dense(64, activation='relu')) #added a dense layer
# Change activation to 'softmax' for multi-class classification
model.add(Dense(4, activation='softmax'))  # Assuming 4 sentiment classes (0, 1, 2, 3) Adjust if needed

# Compile the model with 'categorical_crossentropy' for multi-class
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #changed to sparse_categorical_crossentropy loss

model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

loss, accuracy = model.evaluate(X_test, y_test)
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

# Predict on the test set
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels

# Classification Report
print(classification_report(y_test, y_pred_classes))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_classes)
plt.figure(figsize=(4, 3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le_sentiment.classes_, yticklabels=le_sentiment.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()